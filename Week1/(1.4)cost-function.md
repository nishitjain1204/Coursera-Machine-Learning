Cost Function
=============

We can measure the `accuracy` of our hypothesis function by using a **cost function**. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with `inputs from x's` and the `actual output y's`.

![](https://lh3.googleusercontent.com/OrnEzpo5mYzYehRr-K_Goc2t2vyz9yUro5-d2drtNk0grPhye6pSQNbJkFDdBdgqlhsgNqZjoCUc-Yj_ixUOWIvImHBFAVdq-TVO0zACMRfg8HB-U3qpGSx7-ZyOekGQEeb_hWU4nGo636hoemVSPFBgs14BxL_00u70b6i8-_Acxk642lxQg9MIj1iIn19C21hsS765o7H3ie2Mb1TMx1ETkUfjOmSOnY72ZaguPysKKZrvdgJBSru4Hn36I0X4mSzPGryNa_mMHdlAd_JBeO8xORw3YsEdpVfYjTEKiA8LFVto9V2LVDOlW_oaFq3JCo4KYbz6W6b8wtwYj_kVxiKG6DDiMS0oOmMUnSZ83o3g0nprzQ7UnzU9_C8cY28966VNuLf5u8q-pyqNcS9zgT5qjha9qI4Rw-ZB-V93i_j1kht6BEf0LTeUK_jd6LHqKhmOAhAzhAJBugXiJ6rrzUco7Q4ZvF5SsfN0AyH6pIOhzzKwOBauBQ8KYpVjaX99t54PPySU6pFG0-6Yjm6MKc_5wJiV4Ems7RPapCKo54_aE0Hios4AaSdWvWqEfJwhVLJ1FnPJk9VykkCfWELDFLett5L5qMnGYAY7bowi56K_vG_pqeKqosvvMxB5U7WNRV2C6mBgN3A9RDE81X9ZcFiS-6lt5rwN61kyvOIQXFmt1fmPBfov5XeKHcNr8g=w909-h137-no?authuser=0)

This function is otherwise called the `Squared error function`, or `Mean squared error`. The mean is `halved (1/2)` as a convenience for the computation of the gradient descent, as the derivative term of the square function will `cancel out` the `(1/2)` term. The following image summarizes what the cost function does:

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R2YF5Lj3EeajLxLfjQiSjg_110c901f58043f995a35b31431935290_Screen-Shot-2016-12-02-at-5.23.31-PM.png?expiry=1598400000000&hmac=7tuT-Q_8Rn6n2wBFWV8HUgAsNXcgDPNW8S1x3ZMw2wY)

<h2> Cost Function - Intuition I </h2>


If we try to think of it in visual terms, our training data set is scattered on the `x-y plane`. We are trying to make a `straight line` (defined by <b>h<sub>θ</sub>(x)</b>) which passes through these scattered data points.

Our objective is to get the `best possible line`. The best possible line will be such so that the `average squared vertical distances of the scattered points from the line will be the least`. Ideally, the line should pass through all the points of our training data set. In such a case, the value of <b>J(θ<sub>0</sub>,θ<sub>1</sub>)</b> will be `0`. 


The following example shows the ideal situation where we have a <b>cost function of 0</b>


![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/_B8TJZtREea33w76dwnDIg_3e3d4433e32478f8df446d0b6da26c27_Screenshot-2016-10-26-00.57.56.png?expiry=1598486400000&hmac=-ORmmkzWwodfvPDgIvlqTmD2o4HVIAUf5QscwGV0_Kc)

When <b>θ<sub>1</sub>=1</b> , we get a `slope of 1` which goes through every single data point in our model. Conversely, when <b>θ<sub>1</sub>=0.5</b>  we see the vertical distance from our fit to the data points increase.

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/8guexptSEeanbxIMvDC87g_3d86874dfd37b8e3c53c9f6cfa94676c_Screenshot-2016-10-26-01.03.07.png?expiry=1598486400000&hmac=0iFeT0MM7hHUOjvJDgIJcTKnpd3jXs-cYAN0mMr3uUM)

This increases our cost function to `0.58`. Plotting several other points yields to the following graph:

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/fph0S5tTEeajtg5TyD0vYA_9b28bdfeb34b2d4914d0b64903735cf1_Screenshot-2016-10-26-01.09.05.png?expiry=1598486400000&hmac=FdJ4WuVjgGxpJbLoQejnIaZIVlbjvjMeKU73fcRLHoQ)

Thus as a goal, we should try to minimize the cost function. 


In this case, <b>θ<sub>1</sub>=1 is our global minimum.</b>


