Cost Function
=============

We can measure the `accuracy` of our hypothesis function by using a **cost function**. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with `inputs from x's` and the `actual output y's`.

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/cost_function_1.png)

This function is otherwise called the `Squared error function`, or `Mean squared error`. The mean is `halved (1/2)` as a convenience for the computation of the gradient descent, as the derivative term of the square function will `cancel out` the `(1/2)` term. The following image summarizes what the cost function does:

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/cost_function_2.png)


<h2> Cost Function - Intuition I </h2>


If we try to think of it in visual terms, our training data set is scattered on the `x-y plane`. We are trying to make a `straight line` (defined by <b>h<sub>θ</sub>(x)</b>) which passes through these scattered data points.

Our objective is to get the `best possible line`. The best possible line will be such so that the `average squared vertical distances of the scattered points from the line will be the least`. Ideally, the line should pass through all the points of our training data set. In such a case, the value of <b>J(θ<sub>0</sub>,θ<sub>1</sub>)</b> will be `0`. 


The following example shows the ideal situation where we have a <b>cost function of 0</b>


![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/cost_function_3.png)

When <b>θ<sub>1</sub>=1</b> , we get a `slope of 1` which goes through every single data point in our model. Conversely, when <b>θ<sub>1</sub>=0.5</b>  we see the vertical distance from our fit to the data points increase.

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/cost_function_4.png)

This increases our cost function to `0.58`. Plotting several other points yields to the following graph:

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/cost_function_5.png)

Thus as a goal, we should try to minimize the cost function. 


In this case, <b>θ<sub>1</sub>=1 is our global minimum.</b>


<h2>Cost Function - Intuition II </h2>


A <b>contour plot</b> is a `graph` that contains many contour lines. A `contour line` of a two variable function has a constant value at all points of the same line.

An example of such a graph is the one to the right below.

![]()

Taking any color and going along the `circle`, one would expect to get the same value of the cost function. For example, the `three green points` found on the green line above have the same value for <b>J(θ<sub>0</sub>,θ<sub>1</sub>)</b> and as a result, they are found along the same line. The `circled x` displays the value of the `cost function` for the graph on the left when <b>θ<sub>0</sub> = 800</b> and <b>θ<sub>1</sub>= -0.15</b>. 

Taking another `h(x)` and plotting its contour plot, one gets the following graphs:

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/26RZhJ34EeaiZBL80Yza_A_0f38a99c8ceb8aa5b90a5f12136fdf43_Screenshot-2016-10-29-01.14.57.png?expiry=1598572800000&hmac=jO0tiEkDKvI4HhoJcHDA2yRvAqbhvBP1rylYqwCFNGs)

When <b>θ<sub>0</sub> = 360</b> and <b>θ<sub>1</sub> = 0</b>, the value of <b>J(θ<sub>0</sub>,θ<sub>1</sub>)</b> in the contour plot gets closer to the center thus `reducing the cost function error`.


Now giving our hypothesis function a slightly positive slope results in a better fit of the data.

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/hsGgT536Eeai9RKvXdDYag_2a61803b5f4f86d4290b6e878befc44f_Screenshot-2016-10-29-09.59.41.png?expiry=1598572800000&hmac=IwHyIhdQHvMcUU151pbLRyBHNQG4XvOo7yyqh4NkRv4)

The graph above minimizes the cost function as much as possible and consequently, the result of <b>θ<sub>1</sub></b> and <b>θ<sub>0</sub></b> tend to be around `0.12` and `250` respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most 'circle'.

