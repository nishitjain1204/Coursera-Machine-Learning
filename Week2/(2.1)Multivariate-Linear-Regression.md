Multivariate Linear Regression
=================



Linear regression with `multiple variables` is  known as <b>multivariate linear regression</b> .

We now introduce `notation for equations` where we can have `any number of input variables`.

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_1.png)

The `multivariable form of the hypothesis function` accommodating these multiple features is as follows:

hθ<sub>(x)</sub> = θ<sub>0</sub>+θ<sub>1</sub>x<sub>1</sub>+θ<sub>2</sub>x<sub>2</sub>+θ<sub>3</sub>x<sub>3</sub>+⋯+θ<sub>n</sub>x<sub>n</sub>

In order to develop intuition about this function, we can think about <b>θ<sub>0</sub></b> as the `basic price of a house`, <b>θ<sub>1</sub></b> as the `price per square meter`, <b>θ<sub>2</sub></b> as the `price per floor`, etc. <b>x<sub>1</sub></b> will be the `number of square meters in the house`, <b>x<sub>2</sub></b> the `number of floors`, etc.

Using the definition of `matrix multiplication`, our multivariable hypothesis function can be concisely represented as:


![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_2.png)

This is a `vectorization of our hypothesis function` for one training example .

Remark:

Note that for convenience reasons in this course we <b>assume x<sub>0</sub><sup>(i)</sup>=1 for (i∈1,...,m)</b>. 

This allows us to do matrix operations with theta and x. Hence making the two vectors 'θ' and x<sup>(i)</sup> match each other element-wise (that is, have the same number of elements: n+1).



**Gradient Descent for Multiple Variables**
-------------------------------------------

The gradient descent equation itself is generally `the same form` ; we just have to <b>repeat it for our 'n' features</b> :


![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_3.png)

In other words:

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_4.png)

<b> The following image compares gradient descent with one variable to gradient descent with multiple variables </b>:

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_5.png)



Gradient Descent in Practice I - Feature Scaling
================================================



We can <b>speed up gradient descent</b> by having each of our `input values in roughly the same range`. This is because `θ will descend quickly on small ranges` and `slowly on large ranges`, and so will `oscillate inefficiently down to the optimum` when the variables are very uneven.

The way to prevent this is to `modify the ranges of our input variables` so that they are all roughly the same. Ideally:

<b>
-1 ≤ x<sub>i</sub> ≤ 1

or

-0.5 ≤ x<sub>i</sub> ≤ 0.5
</b>


These <b>aren't exact requirements</b> ; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.

Two techniques to help with this are `feature scaling` and `mean normalization`. 

<b>Feature scaling</b> involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. 

<b>Mean normalization</b> involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. 

To implement both of these techniques, adjust your input values as shown in this formula:

 x<sub>i</sub> : =  ( x<sub>i</sub> -  μ<sub>i</sub> ) / s<sub>i</sub>

Where <b>μ<sub>i</sub></b> is the `average` of all the values for feature (i) and <b>s<sub>i</sub></b> is the range of values (max - min), or <b>s<sub>i</sub></b> is the standard deviation.

**Note that dividing by the range, or dividing by the standard deviation, give different results.** 


For example, if <b>x<sub>i</sub></b> represents housing prices with a `range of 100 to 2000` and a `mean value of 1000` , then, <b> x<sub>i</sub> := (price-1000)/1900 </b> .




Gradient Descent in Practice II - Learning Rate
===============================================



**Debugging gradient descent.**

Make a plot with `number of iterations` on the x-axis. Now plot the `cost function` , `J(θ)` over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease `α`.

**Automatic convergence test.**

`Declare convergence` if J(θ) decreases by **less than E** in one iteration , where `E is some small value` such as 10<sup>-3</sup>. However in practice it's difficult to choose this threshold value.

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_6.png)

It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_7.png)


To summarize:

<b>If α is too small </b>: slow convergence.

**If α is too large** :J(θ) may not decrease on every iteration and thus may not converge.



Features and Polynomial Regression
==================================

We can improve our features and the form of our hypothesis function in a couple different ways.

We can **combine** multiple features into one. For example, we can combine x<sub>1</sub> and x<sub>2</sub> into a new feature x<sub>3</sub> by taking x<sub>1</sub> * x<sub>2</sub>

### **Polynomial Regression**

Our hypothesis function need not be linear (a straight line) if that does not fit the data well.

We can **change the behavior or curve** of our hypothesis function by making it a `quadratic` , `cubic` or `square root` function (or any other form).

For example, if our hypothesis function is <b> h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> </b> then we can create additional features based on <b> x<sub>1</sub> </b> , to get the quadratic function <b> h<sub>θ</sub>(x)=θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> +θ<sub>2</sub>x<sub>1</sub><sup>2</sup> </b> or the cubic function <b> h<sub>θ</sub>(x)=θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> +θ<sub>2</sub>x<sub>1</sub><sup>2</sup> + θ<sub>3</sub>x<sub>1</sub><sup>3</sup> </b>

In the cubic version, we have created new features <b> x<sub>2</sub> </b> and <b> x<sub>3</sub> </b> where  <b> x2=x<sub>1</sub><sup>2</sup> </b> and <b> x<sub>3</sub> = x<sub>1</sub><sup>3</sup> </b>

To make it a square root function, we could do: 
<b>h<sub>θ</sub>(x)=θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> + θ<sub>2</sub> * sqrt(x<sub>1</sub>) </b>


One important thing to keep in mind is, if you choose your features this way then `feature scaling` becomes very important.

eg. if <b>x<sub>1</sub></b> has range `1 - 1000` then range of <b>x<sub>1</sub><sup>2</sup></b> becomes `1 - 1000000` and that of <b>x<sub>1</sub><sup>3</sup></b> becomes `1 - 1000000000`
