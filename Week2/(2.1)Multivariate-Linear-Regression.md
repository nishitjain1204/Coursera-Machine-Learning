Multivariate Linear Regression
=================



Linear regression with `multiple variables` is  known as <b>multivariate linear regression</b> .

We now introduce notation for equations where we can have any number of input variables.

![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_1.png)

The multivariable form of the hypothesis function accommodating these multiple features is as follows:

hθ<sub>(x)</sub> = θ<sub>0</sub>+θ<sub>1</sub>x<sub>1</sub>+θ<sub>2</sub>x<sub>2</sub>+θ<sub>3</sub>x<sub>3</sub>+⋯+θ<sub>n</sub>x<sub>n</sub>

In order to develop intuition about this function, we can think about θ<sub>0</sub> as the basic price of a house, θ<sub>1</sub> as the price per square meter, θ<sub>2</sub> as the price per floor, etc. x<sub>1</sub> will be the number of square meters in the house, x<sub>2</sub> the number of floors, etc.

Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:


![](https://github.com/coldkillerr/Coursera-Machine-Learning/blob/master/images/Multivariate_Regression_2.png)

This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.

Remark: Note that for convenience reasons in this course we assume x<sub>0</sub><sup>(i)</sup>=1 for (i∈1,...,m). This allows us to do matrix operations with theta and x. Hence making the two vectors 'θ' and x<sup>(i)</sup> match each other element-wise (that is, have the same number of elements: n+1).]
